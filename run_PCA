def run_PCA(activations_collect,key_idx, components, threshold=0.999):
        """threshold for minimal loss in performance=0.999
        activations_collect  function gathers activations over enough mini batches.
        components=number of filters in the layer you are compressing"""
        
        print('number of components are',components)
        activations=activations_collect[key_idx]#.replace('.weight','')] 
        print('shape of activations are:',activations.shape)
        a=activations.swapaxes(1,2).swapaxes(2,3)
        a_shape=a.shape
        print('reshaped ativations are of shape',a.shape)
        pca = PCA(n_components=components) #number of components should be equal to the number of filters
        pca.fit(a.reshape(a_shape[0]*a_shape[1]*a_shape[2],a_shape[3])) #this should be N*H*W,M
        a_trans=pca.transform(a.reshape(a_shape[0]*a_shape[1]*a_shape[2],a_shape[3]))
        print('explained variance ratio is:',pca.explained_variance_ratio_)
        plt.plot(np.cumsum(pca.explained_variance_ratio_))
        plt.savefig('./PCA_files_'+key_idx+'.jpeg')  #saves the PCA figure.
        optimal_num_filters=np.sum(np.cumsum(pca.explained_variance_ratio_)<threshold) 
        print('we want to retain this percentage of explained variance',threshold)
        print('number of filters required to explain that much variance is',optimal_num_filters)
return optimal_num_filters,pca.components_
